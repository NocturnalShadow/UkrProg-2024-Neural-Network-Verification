{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GC is disabled\n"
     ]
    }
   ],
   "source": [
    "# Imports and configs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pickle import dump, load\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "import math\n",
    "\n",
    "import re\n",
    "\n",
    "from z3 import *\n",
    "from functools import partial\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Pandas-Jupyter options\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "pd.set_option(\"display.max_rows\", 6)\n",
    "pd.set_option(\"display.max_columns\", 90)\n",
    "pd.set_option(\"display.float_format\", '{:,.2g}'.format)\n",
    "\n",
    "# Disable SKLearn warnings\n",
    "import warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "\n",
    "import gc\n",
    "gc.disable()\n",
    "gc.collect()\n",
    "print(\"GC is enabled\" if gc.isenabled() else \"GC is disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "# Filter flows\n",
    "def filter_flows(flows):\n",
    "    recognized_types = [\"BENIGN\", \"Virut\", \"Neris\", \"RBot\", \"IRC attack\", \"Murlo\", \"Tbot\", \"Sogou\"]\n",
    "    flows = flows[flows[\"FlowType\"].isin(recognized_types)]\n",
    "    flows = flows[flows[\"Protocol\"].isin([\"TCP\", \"UDP\", \"ICMP\"])]\n",
    "    flows = flows.replace(np.nan, 0).replace(np.inf, 0)\n",
    "    flows = flows[flows[\"Flow Duration\"] >= 0]\n",
    "    return flows\n",
    "\n",
    "def process(flows):\n",
    "    non_features = [\"Flow ID\", \"Src IP\", \"Src Port\", \"Dst IP\", \"Dst Port\", \"Timestamp\", \"HostType\", \"FlowType\", \"Protocol\", \"FlowTerminationReason\",]\n",
    "    flows = flows[flows.columns.drop(list(flows.filter(regex='Idle.*|Active.*|.*/s|^Flow IAT.*|^Packet Length.*|.*Mean.*|.*Average.*')))] # Idle.*|Active.*|.*/s'\n",
    "    flows_features = flows[flows.columns.drop(non_features)]\n",
    "    flow_labels = flows[\"FlowType\"].replace(to_replace=r\"^(.(?<!BENIGN))*?$\", value=\"MALICIOUS\",regex=True).replace({ \"BENIGN\": 0, \"MALICIOUS\": 1 })\n",
    "    flows_features = flows_features.assign(**flows_features[[\"Total Length of Fwd Packet\", \"Total Length of Bwd Packet\"]].astype(np.int64))\n",
    "    return flows_features, flow_labels, flows\n",
    "\n",
    "# Load flows\n",
    "flows_train = pd.read_csv('../ISCX_Botnet-Training.pcap_Flow.csv', index_col=False)\n",
    "flows_test = pd.read_csv('../ISCX_Botnet-Testing.pcap_Flow.csv', index_col=False)\n",
    "\n",
    "flows_train = filter_flows(flows_train)\n",
    "flows_test = filter_flows(flows_test)\n",
    "\n",
    "train_features, train_labels, train_flows = process(flows_train)\n",
    "test_features, test_labels, test_flows = process(flows_test)\n",
    "\n",
    "train_malicious_flows = train_flows[train_flows[\"FlowType\"] != \"BENIGN\"]\n",
    "train_malicious_features = train_features.loc[train_malicious_flows.index]\n",
    "train_malicious_labels = train_labels.loc[train_malicious_flows.index]\n",
    "\n",
    "train_benign_flows = train_flows[train_flows[\"FlowType\"] == \"BENIGN\"]\n",
    "train_benign_features = train_features.loc[train_benign_flows.index]\n",
    "train_benign_labels = train_labels.loc[train_benign_flows.index]\n",
    "\n",
    "test_malicious_flows = test_flows[test_flows[\"FlowType\"] != \"BENIGN\"]\n",
    "test_malicious_features = test_features.loc[test_malicious_flows.index]\n",
    "test_malicious_labels = test_labels.loc[test_malicious_flows.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Scaler (set feature count)\n",
    "feature_count = 50\n",
    "scaler = None\n",
    "\n",
    "def scale_data(flows_features, scaler_file=f'archive_{feature_count}/scaler.pkl', load_scaler=False):\n",
    "    global scaler\n",
    "    if not scaler:\n",
    "        if load_scaler:\n",
    "            scaler = load(open(scaler_file, 'rb'))\n",
    "        else:\n",
    "            scaler = MinMaxScaler().fit(flows_features)\n",
    "            dump(scaler, open(scaler_file, 'wb'))\n",
    "    return scaler.transform(flows_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mal.: 109305 samples\n",
      "Train correct mal. (0.5 < conf. < 0.55): 836 samples (0.76 percent of total)\n",
      "Test mal.: 87960 samples\n",
      "Test correct mal. (0.5 < conf. < 0.55): 351 samples (0.40 percent of total)\n"
     ]
    }
   ],
   "source": [
    "# Split sample by classification result (set threshold)\n",
    "threshold = 0.5\n",
    "margin = 0.05\n",
    "\n",
    "# Get correctly classified\n",
    "def get_correct_samples_with_margin(model, features, labels, threshold, margin):\n",
    "    # Scale features\n",
    "    scaled_features = scale_data(features)\n",
    "\n",
    "    # Predict\n",
    "    predictions = model.predict(scaled_features, verbose=0)\n",
    "    prediction_labels = [math.ceil(prediction[0]) if prediction[0] > threshold else math.floor(prediction[0]) for prediction in predictions]\n",
    "    selected = [True if prediction[0] > threshold and prediction[0] < threshold + margin else False for prediction in predictions]\n",
    "    \n",
    "    labels = labels.copy().to_frame()\n",
    "    labels[\"PredictedFlowType\"] = prediction_labels\n",
    "    correct_labels = labels[selected][labels[\"FlowType\"] == labels[\"PredictedFlowType\"]]\n",
    "    correct_features = features[selected].loc[correct_labels.index]\n",
    "    return correct_features, correct_labels.drop(columns=[\"PredictedFlowType\"])\n",
    "\n",
    "model = tf.keras.models.load_model(\"dnn_base.h5\")\n",
    "\n",
    "train_correct_features, train_correct_labels = get_correct_samples_with_margin(model, train_features, train_labels, threshold, margin)\n",
    "test_correct_features, test_correct_labels = get_correct_samples_with_margin(model, test_features, test_labels, threshold, margin)\n",
    "\n",
    "test_correct_malicious_labels = test_correct_labels[test_correct_labels.index.isin(test_malicious_labels.index)]\n",
    "test_correct_malicious_features = test_correct_features.loc[test_correct_malicious_labels.index]\n",
    "\n",
    "train_correct_malicious_labels = train_correct_labels[train_correct_labels.index.isin(train_malicious_labels.index)]\n",
    "train_correct_malicious_features = train_correct_features.loc[train_correct_malicious_labels.index]\n",
    "\n",
    "print(\"Train mal.: %s samples\" % len(train_malicious_features))\n",
    "print(\"Train correct mal. (%s < conf. < %s): %s samples (%.2f percent of total)\" % (threshold, threshold + margin, len(train_correct_malicious_features), len(train_correct_malicious_features) / len(train_malicious_features)*100))\n",
    "\n",
    "print(\"Test mal.: %s samples\" % len(test_malicious_features))\n",
    "print(\"Test correct mal. (%s < conf. < %s): %s samples (%.2f percent of total)\" % (threshold, threshold + margin, len(test_correct_malicious_features), len(test_correct_malicious_features) / len(test_malicious_features)*100))\n",
    "\n",
    "# SANITY CHECK\n",
    "# Check if for all features prediction is greater than threshold and less than margin\n",
    "def confidence_margin(model, features, threshold, margin):\n",
    "    scaled_features = scale_data(features)\n",
    "    predictions = model.predict(scaled_features, verbose=0)\n",
    "    selected = [True if prediction[0] > threshold and prediction[0] < threshold + margin else False for prediction in predictions]\n",
    "    return all(selected)\n",
    "\n",
    "assert confidence_margin(model, train_correct_malicious_features, threshold, margin)\n",
    "assert confidence_margin(model, test_correct_malicious_features, threshold, margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifier\n",
    "\n",
    "# Constants\n",
    "ReLU = 0\n",
    "Linear = 1\n",
    "Const0 = 2\n",
    "Unknown = -1\n",
    "\n",
    "inv_sigmoid = np.vectorize(lambda x: np.log(x) - np.log(1 - x))\n",
    "z3_real = np.vectorize(Real)\n",
    "z3_real_val = np.vectorize(RealVal)\n",
    "\n",
    "def _z3_relu(x):\n",
    "    return If(x > 0, x, 0)\n",
    "\n",
    "z3_relu = np.vectorize(_z3_relu)\n",
    "z3_simplify = np.vectorize(_z3_relu)\n",
    "\n",
    "# -- Based on previous layer bounds calculates the bounds for the next layer (without activation)\n",
    "def _next_layer_bounds(weights, biases, input_bounds):\n",
    "    next_min_bounds = []\n",
    "    next_max_bounds = []\n",
    "    prev_count, next_count = weights.shape\n",
    "    min_bounds, max_bounds = input_bounds\n",
    "    # for each neuron in the next layer compute the bounds\n",
    "    for i in range(next_count):\n",
    "        # compute the bounds for the i-th neuron\n",
    "        next_min_bound = 0\n",
    "        next_max_bound = 0\n",
    "        for j in range(prev_count):\n",
    "            if weights[j][i] > 0:\n",
    "                next_min_bound += weights[j][i] * min_bounds[j]\n",
    "                next_max_bound += weights[j][i] * max_bounds[j]\n",
    "            else:\n",
    "                next_min_bound += weights[j][i] * max_bounds[j]\n",
    "                next_max_bound += weights[j][i] * min_bounds[j]\n",
    "        next_min_bound += biases[i]\n",
    "        next_max_bound += biases[i]\n",
    "        next_min_bounds.append(next_min_bound)\n",
    "        next_max_bounds.append(next_max_bound)\n",
    "\n",
    "    return next_min_bounds, next_max_bounds\n",
    "\n",
    "def _optimize_layer_bounds(layer_activation, input_bounds):\n",
    "    min_bounds, max_bounds = input_bounds\n",
    "    for j in range(len(min_bounds)):\n",
    "        if layer_activation[j] == ReLU:\n",
    "            if min_bounds[j] >= 0:\n",
    "                layer_activation[j] = Linear\n",
    "            elif max_bounds[j] <= 0:\n",
    "                layer_activation[j] = Const0\n",
    "            min_bounds[j] = max(min_bounds[j], 0)\n",
    "            max_bounds[j] = max(max_bounds[j], 0)\n",
    "        elif layer_activation[j] == Linear:\n",
    "            min_bounds[j] == 0\n",
    "        elif layer_activation[j] == Const0:\n",
    "            min_bounds[j] = 0\n",
    "            max_bounds[j] = 0\n",
    "\n",
    "def _create_activation_matrix(model, skip_output_activation=True):\n",
    "    activation = []\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if layer.activation == tf.keras.activations.relu:\n",
    "            activation.append([ReLU] * layer.output_shape[1])\n",
    "        elif layer.activation == tf.keras.activations.linear:\n",
    "            activation.append([Linear] * layer.output_shape[1])\n",
    "        else:\n",
    "            if skip_output_activation and layer == model.layers[-1]:\n",
    "                activation.append([Linear] * layer.output_shape[1])\n",
    "            else:\n",
    "                print(\"ERROR: Activation function is not supported!\")\n",
    "                activation.append([Linear] * layer.output_shape[1])\n",
    "    return activation\n",
    "\n",
    "def _optimize_layer_activations(layer_activation, X, x, constraints):\n",
    "    # 1. Compute neurons value for the current layer based on the previous one without activation\n",
    "    # 2. If the value > 0, check if on the input range it is always > 0:\n",
    "    #    if yes, then consider the neuron activation as linear\n",
    "    #    if no, then neuron activation is ReLU\n",
    "    # 3. If the value < 0, check if on the input range it is always < 0\n",
    "    #    if yes, then consider the neuron activation as constant 0\n",
    "    #    if no, then neuron activation is ReLU\n",
    "    \n",
    "    # Build optimized z3 representation of the current layer\n",
    "    # MAYBE: reuse the boundaries known for the previous layer\n",
    "    # TODO: parallelize the SMT tasks for each neuron\n",
    "    for i in range(len(X)):\n",
    "        if layer_activation[i] == ReLU:\n",
    "            s = Solver()\n",
    "            for constraint in constraints:\n",
    "                s.add(constraint)\n",
    "            if x[i] > 0:\n",
    "                # Check if on the input range it is always > 0:\n",
    "                s.add(Not(X[i] >= 0))\n",
    "                if s.check() == unsat:\n",
    "                    layer_activation[i] = Linear\n",
    "                    # MAYBE: this \"> 0\" fact can later be used to optimize the next layer\n",
    "                else:\n",
    "                    X[i] = z3_relu(X[i])\n",
    "                    x[i] = max(x[i], 0)\n",
    "            elif x[i] < 0:\n",
    "                # Check if on the input range it is always < 0\n",
    "                s.add(Not(X[i] <= 0))\n",
    "                if s.check() == unsat:\n",
    "                    X[i] = RealVal(0.) # Consider the neuron activation as constant 0\n",
    "                    x[i] = 0\n",
    "                    layer_activation[i] = Const0\n",
    "                else:\n",
    "                    X[i] = z3_relu(X[i])\n",
    "                    x[i] = max(x[i], 0)\n",
    "            else:\n",
    "                print(\"Unexpected!\")\n",
    "                X[i] = z3_relu(X[i])\n",
    "                x[i] = max(x[i], 0)\n",
    "        if layer_activation[i] == Const0:\n",
    "            X[i] = RealVal(0.)\n",
    "            x[i] = 0\n",
    "        elif layer_activation[i] == Unknown:\n",
    "            print(\"Activation function is not supported\")\n",
    "\n",
    "    return input, constraints\n",
    "\n",
    "def _absolute_eps_ball_constraints(x, eps, input, feat_indexes, freeze_zero_features=True):\n",
    "    # TODO: handle case when eps > 1\n",
    "    # TODO: handle case when x[i] == 0\n",
    "    # NOTE: here we assumes features are in [0, +inf] range. Remove \"max\" and/or add clamping if needed otherwise\n",
    "    if feat_indexes is None:\n",
    "        constraints = [And(RealVal(max(x[i] - eps, 0)) < input[i], input[i] < RealVal(x[i] + eps)) for i in range(len(x))]\n",
    "    else:\n",
    "        if freeze_zero_features:\n",
    "            constraints = [\n",
    "                And(input[i] >= RealVal(max(x[i] - eps, 0)), input[i] <= RealVal(x[i] + eps)) \n",
    "                if x[i] != 0 \n",
    "                else input[i] == RealVal(0.) \n",
    "                for i in range(len(x))\n",
    "            ]\n",
    "        else:         \n",
    "            constraints = [\n",
    "                And(input[i] >= RealVal(max(x[i] - eps, 0)), input[i] <= RealVal(x[i] + eps)) \n",
    "                for i in feat_indexes\n",
    "            ]\n",
    "\n",
    "    return constraints    \n",
    "\n",
    "def _relative_eps_ball_constraints(x, eps, input, feat_indexes):\n",
    "    # TODO: handle case when eps > 1\n",
    "    if feat_indexes is None:\n",
    "        constraints = [And(x[i] * RealVal(1 - eps) <= input[i], input[i] <= x[i] * RealVal(1 + eps)) for i in range(len(x))]\n",
    "    else:\n",
    "        # NOTE: No epsilon-ball for 0-valued features\n",
    "        constraints = [\n",
    "            And(input[i] >= RealVal(x[i] * (1 - eps)), input[i] <= RealVal(x[i] * (1 + eps)))\n",
    "            for i in feat_indexes\n",
    "        ]\n",
    "    return constraints\n",
    "\n",
    "# Compute input bounds (asumes inputs are non-negative)\n",
    "def _absolute_eps_input_bounds(x, eps, feat_indexes):\n",
    "    if feat_indexes is None:\n",
    "        min_bounds, max_bounds = np.maximum(0, x - eps), x + eps\n",
    "    else:\n",
    "        min_bounds, max_bounds = x.copy(), x.copy()\n",
    "        min_bounds[feat_indexes] = np.maximum(0, min_bounds[feat_indexes] - eps)\n",
    "        max_bounds[feat_indexes] += eps\n",
    "    return min_bounds, max_bounds\n",
    "\n",
    "def _relative_eps_input_bounds(x, eps, feat_indexes):\n",
    "    # TODO: handle case when eps > 1\n",
    "    if feat_indexes is None:\n",
    "        min_bounds, max_bounds = x * (1 - eps), x * (1 + eps)\n",
    "    else:\n",
    "        min_bounds, max_bounds = x.copy(), x.copy()\n",
    "        min_bounds[feat_indexes] *= (1 - eps)\n",
    "        max_bounds[feat_indexes] *= (1 + eps)\n",
    "    return min_bounds, max_bounds\n",
    "\n",
    "def _sample_feasible_inputs(input, constraints, count):\n",
    "    samples = []\n",
    "    solver = SolverFor(\"QF_LRA\")\n",
    "    solver.add(constraints)\n",
    "\n",
    "    for _ in range(count):\n",
    "        if solver.check() == sat:\n",
    "            m = solver.model()\n",
    "            sample = np.array([\n",
    "                float(m[i].as_fraction()) \n",
    "                if isinstance(m[i], RatNumRef) \n",
    "                else float(i.as_fraction())\n",
    "                for i in input\n",
    "            ])\n",
    "            samples.append(sample)\n",
    "            # Add a new constraint that blocks the current solution\n",
    "            solver.add(Or([d() != m[d] for d in m.decls()]))\n",
    "        else:\n",
    "            assert False, \"Constraints are infeasible\"\n",
    "            break\n",
    "    return samples  \n",
    "\n",
    "def z3_var(name):\n",
    "    # Replace all non-alphanumeric characters with underscores\n",
    "    name = re.sub(r'\\W', '_', name).lower()\n",
    "    return Real(name)\n",
    "\n",
    "# -- Produces Z3 representation of the DNN model optimized around the given input\n",
    "def to_z3_graph_optimized_around(model, input, x, eps, constraints, input_bounds, skip_output_activation=True, eps_ball_relative=True):\n",
    "    # Construct activation matrix for each layer\n",
    "    activation = _create_activation_matrix(model, skip_output_activation)\n",
    "\n",
    "    X = input\n",
    "    for layer_index, layer in enumerate(model.layers):\n",
    "        params = layer.get_weights()\n",
    "        \n",
    "        # Layer numerical representation (no activation yet)\n",
    "        w = params[0]\n",
    "        b = params[1]\n",
    "        x = x @ w + b\n",
    "\n",
    "        # Layer Z3 representation (no activation yet)\n",
    "        W = z3_real_val(w)\n",
    "        B = z3_real_val(b)\n",
    "        X = X @ W + B\n",
    "\n",
    "        input_bounds = _next_layer_bounds(w, b, input_bounds)\n",
    "        _optimize_layer_bounds(activation[layer_index], input_bounds)\n",
    "\n",
    "        if skip_output_activation and layer == model.layers[-1]:\n",
    "            continue\n",
    "        \n",
    "        _optimize_layer_activations(activation[layer_index], X, x, constraints)\n",
    "    \n",
    "    return X, activation\n",
    "\n",
    "samples = []\n",
    "x_d = None\n",
    "input = None\n",
    "def eps_ball_verify(model, X, expected_label, eps, threshold, features, free_features, extra_constraints=[], eps_ball_relative=True, debug=True):\n",
    "    # TODO: add timer\n",
    "    # TODO: try parallellizing the SMT tasks for neurons in the same layer\n",
    "\n",
    "    global samples\n",
    "    global x_d\n",
    "    global input\n",
    "\n",
    "    feat_indexes = [features.get_loc(f) for f in free_features] if free_features else None\n",
    "    batch_size = 25\n",
    "    result = [None] * len(X)\n",
    "    relu_count = []\n",
    "    linear_count = []\n",
    "    const_count = []\n",
    "    for i, x in enumerate(X):\n",
    "        x_d = x\n",
    "        if feat_indexes is None:\n",
    "            input = z3_real([z3_var(f) for f in features])\n",
    "        else:\n",
    "            input = [z3_var(f) if f in free_features else RealVal(x[f_i]) for f_i, f in enumerate(features)]\n",
    "\n",
    "        if eps_ball_relative:\n",
    "            constraints = _relative_eps_ball_constraints(x, eps, input, feat_indexes)\n",
    "            input_bounds = _relative_eps_input_bounds(x, eps, feat_indexes)\n",
    "        else:\n",
    "            constraints = _absolute_eps_ball_constraints(x, eps, input, feat_indexes)\n",
    "            input_bounds = _absolute_eps_input_bounds(x, eps, feat_indexes)\n",
    "\n",
    "        constraints += extra_constraints\n",
    "        output, activation = to_z3_graph_optimized_around(model, input, x, eps, constraints, input_bounds, eps_ball_relative)\n",
    "\n",
    "        # Count ReLU activations after optimization (just for statistics)\n",
    "        relu_count.append(sum([sum([1 for a in layer if a == ReLU]) for layer in activation]))\n",
    "\n",
    "        s = SolverFor(\"QF_LRA\")\n",
    "        # s = Solver()\n",
    "        s.add(constraints)\n",
    "        s.add(Not(output[0] > inv_sigmoid(threshold) if expected_label == 1 else output[0] <= inv_sigmoid(threshold)))\n",
    "        result[i] = s.check() == unsat\n",
    "\n",
    "        if i != 0 and i % batch_size == 0:\n",
    "            print(f\"Processed {i} samples (eps = {eps}): {sum(result[:i])} verified, {i - sum(result[:i])} failed\")\n",
    "\n",
    "        if debug:\n",
    "            # === SANITY ASSERTIONS === \n",
    "            tolerance = 2e-5\n",
    "            if not result[i]:\n",
    "                m = s.model()\n",
    "                if feat_indexes is None:\n",
    "                    counterexample = np.array([float(m[var].as_fraction()) for var in input])\n",
    "                else:\n",
    "                    counterexample = x.copy()\n",
    "                    for feature in feat_indexes:\n",
    "                        counterexample[feature] = float(m[input[feature]].as_fraction())\n",
    "                \n",
    "                tf_out = model.predict(np.expand_dims(counterexample, axis=0), verbose=0)[0][0]\n",
    "                z3_out = tf.sigmoid(float(m.evaluate(output[0]).as_fraction())).numpy()\n",
    "\n",
    "                if expected_label == 1:\n",
    "                    assert tf_out <= threshold + tolerance, f\"TF output exceeds threshold {tf_out} > {threshold}\"\n",
    "                else:\n",
    "                    assert tf_out + tolerance > threshold, f\"TF output doesn't exceed threshold {tf_out} <= {threshold}\"\n",
    "\n",
    "                assert abs(tf_out - z3_out) < tolerance, f\"TF and Z3 ouputs does not match: {tf_out} != {z3_out}\"\n",
    "            else:\n",
    "                # take several random points from the input constraints space and check that they are classified correctly\n",
    "                # perturb x by a small amount and check that it is classified correctly\n",
    "                # TODO: IMPORTANT! fix inter-feature scaling\n",
    "                sample_count = 0\n",
    "                samples = _sample_feasible_inputs(input, constraints, count=sample_count)\n",
    "                \n",
    "                for sample in samples:\n",
    "                    tf_out = model.predict(np.expand_dims(sample, axis=0), verbose=0)[0][0]\n",
    "                    if expected_label == 1:\n",
    "                        assert tf_out + tolerance > threshold, \"Verification passed, but a perturbed sample was miscalssified\"\n",
    "                    else:\n",
    "                        assert tf_out < threshold + tolerance, \"Verification passed, but a perturbed sample was miscalssified\"\n",
    "\n",
    "    avg_relu_count = sum(relu_count) / len(relu_count)\n",
    "    print(f\"Processed {len(X)} samples (eps = {eps}): {sum(result)} verified, {len(X) - sum(result)} failed; Avg. ReLU count: {avg_relu_count}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(feature, feature_name):\n",
    "    # Get the index of the feature\n",
    "    features = scaler.get_feature_names_out()\n",
    "    \n",
    "    # Find the index of feature_name in feature_index\n",
    "    index = np.where(features == feature_name)\n",
    "    min_val = scaler.data_min_[index].flatten()[0]\n",
    "    max_val = scaler.data_max_[index].flatten()[0]\n",
    "\n",
    "    return feature * (max_val - min_val) + min_val\n",
    "\n",
    "def normalize(feature, feature_name):\n",
    "    # Get the index of the feature\n",
    "    features = scaler.get_feature_names_out()\n",
    "    \n",
    "    # Find the index of feature_name in feature_index\n",
    "    index = np.where(features == feature_name)\n",
    "    min_val = scaler.data_min_[index].flatten()[0]\n",
    "    max_val = scaler.data_max_[index].flatten()[0]\n",
    "    return (feature - min_val) / (max_val - min_val)\n",
    "\n",
    "# Test normalization\n",
    "# for i in range(len(train_features)):\n",
    "#     scaled_value_1 = scaler.transform([train_features.iloc[i]])[0][0]\n",
    "#     scaled_value_2 = normalize(train_features.iloc[i][\"Flow Duration\"], \"Flow Duration\")\n",
    "#     print(i)\n",
    "#     assert abs(scaled_value_1 - scaled_value_2) < 1e-9, f\"Normalization failed: {scaled_value_1} != {scaled_value_2}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define free features and constraints\n",
    "MALICIOUS = 1\n",
    "BENIGN = 0\n",
    "\n",
    "free_features = [\n",
    "    \"Flow Duration\",\n",
    "    # \"Fwd Packet Length Max\", \"Fwd Packet Length Min\", \"Fwd Packet Length Std\",\n",
    "    # \"Bwd Packet Length Max\", \"Bwd Packet Length Min\", \"Bwd Packet Length Std\",\n",
    "    \"Fwd IAT Total\", \"Fwd IAT Std\", \"Fwd IAT Max\", \"Fwd IAT Min\", \n",
    "    \"Bwd IAT Total\", \"Bwd IAT Std\", \"Bwd IAT Max\", \"Bwd IAT Min\"\n",
    "]\n",
    "\n",
    "# constraints = [\n",
    "#     z3_var(\"Fwd IAT Max\") >= z3_var(\"Fwd IAT Min\"),\n",
    "#     z3_var(\"Bwd IAT Max\") >= z3_var(\"Bwd IAT Min\")\n",
    "# ]\n",
    "\n",
    "constraints = [\n",
    "    denormalize(z3_var(\"Fwd IAT Max\"), \"Fwd IAT Max\") >= denormalize(z3_var(\"Fwd IAT Min\"), \"Fwd IAT Min\"),\n",
    "    denormalize(z3_var(\"Bwd IAT Max\"), \"Fwd IAT Max\") >= denormalize(z3_var(\"Bwd IAT Min\"), \"Bwd IAT Min\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 25 samples (eps = 0.1): 21 verified, 4 failed\n",
      "Processed 50 samples (eps = 0.1): 38 verified, 12 failed\n",
      "Processed 75 samples (eps = 0.1): 54 verified, 21 failed\n",
      "Processed 100 samples (eps = 0.1): 73 verified, 27 failed\n",
      "Processed 125 samples (eps = 0.1): 94 verified, 31 failed\n",
      "Processed 150 samples (eps = 0.1): 118 verified, 32 failed\n",
      "Processed 175 samples (eps = 0.1): 138 verified, 37 failed\n",
      "Processed 200 samples (eps = 0.1): 160 verified, 40 failed\n",
      "Processed 225 samples (eps = 0.1): 177 verified, 48 failed\n",
      "Processed 250 samples (eps = 0.1): 198 verified, 52 failed\n",
      "Processed 275 samples (eps = 0.1): 219 verified, 56 failed\n",
      "Processed 300 samples (eps = 0.1): 239 verified, 61 failed\n",
      "Processed 325 samples (eps = 0.1): 260 verified, 65 failed\n",
      "Processed 350 samples (eps = 0.1): 283 verified, 67 failed\n",
      "Processed 375 samples (eps = 0.1): 306 verified, 69 failed\n",
      "Processed 400 samples (eps = 0.1): 330 verified, 70 failed\n",
      "Processed 425 samples (eps = 0.1): 353 verified, 72 failed\n",
      "Processed 450 samples (eps = 0.1): 376 verified, 74 failed\n",
      "Processed 475 samples (eps = 0.1): 399 verified, 76 failed\n",
      "Processed 500 samples (eps = 0.1): 422 verified, 78 failed\n",
      "Processed 525 samples (eps = 0.1): 440 verified, 85 failed\n",
      "Processed 550 samples (eps = 0.1): 461 verified, 89 failed\n",
      "Processed 575 samples (eps = 0.1): 482 verified, 93 failed\n",
      "Processed 600 samples (eps = 0.1): 507 verified, 93 failed\n",
      "Processed 625 samples (eps = 0.1): 529 verified, 96 failed\n",
      "Processed 650 samples (eps = 0.1): 550 verified, 100 failed\n",
      "Processed 675 samples (eps = 0.1): 573 verified, 102 failed\n",
      "Processed 700 samples (eps = 0.1): 595 verified, 105 failed\n",
      "Processed 725 samples (eps = 0.1): 619 verified, 106 failed\n",
      "Processed 750 samples (eps = 0.1): 644 verified, 106 failed\n",
      "Processed 775 samples (eps = 0.1): 667 verified, 108 failed\n",
      "Processed 800 samples (eps = 0.1): 688 verified, 112 failed\n",
      "Processed 825 samples (eps = 0.1): 712 verified, 113 failed\n",
      "Processed 836 samples (eps = 0.1): 722 verified, 114 failed; Avg. ReLU count: 0.48086124401913877\n"
     ]
    }
   ],
   "source": [
    "# Verify a set of points for base model\n",
    "model = tf.keras.models.load_model(\"dnn_base.h5\")\n",
    "epsilon = 0.1\n",
    "\n",
    "result = eps_ball_verify(\n",
    "    model=model,\n",
    "    X=scale_data(train_correct_malicious_features),\n",
    "    expected_label=MALICIOUS,\n",
    "    eps=epsilon,\n",
    "    threshold=threshold,\n",
    "    features=train_correct_malicious_features.columns,\n",
    "    free_features=free_features,\n",
    "    extra_constraints=constraints, \n",
    "    debug=True)\n",
    "\n",
    "_ = gc.collect()\n",
    "# 15m 7.5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 25 samples (eps = 0.05): 21 verified, 4 failed\n",
      "Processed 50 samples (eps = 0.05): 38 verified, 12 failed\n",
      "Processed 75 samples (eps = 0.05): 54 verified, 21 failed\n",
      "Processed 100 samples (eps = 0.05): 73 verified, 27 failed\n",
      "Processed 125 samples (eps = 0.05): 96 verified, 29 failed\n",
      "Processed 150 samples (eps = 0.05): 120 verified, 30 failed\n",
      "Processed 175 samples (eps = 0.05): 140 verified, 35 failed\n",
      "Processed 200 samples (eps = 0.05): 163 verified, 37 failed\n",
      "Processed 225 samples (eps = 0.05): 181 verified, 44 failed\n",
      "Processed 250 samples (eps = 0.05): 202 verified, 48 failed\n",
      "Processed 275 samples (eps = 0.05): 223 verified, 52 failed\n",
      "Processed 300 samples (eps = 0.05): 244 verified, 56 failed\n",
      "Processed 325 samples (eps = 0.05): 267 verified, 58 failed\n",
      "Processed 350 samples (eps = 0.05): 292 verified, 58 failed\n",
      "Processed 375 samples (eps = 0.05): 316 verified, 59 failed\n",
      "Processed 400 samples (eps = 0.05): 340 verified, 60 failed\n",
      "Processed 425 samples (eps = 0.05): 365 verified, 60 failed\n",
      "Processed 450 samples (eps = 0.05): 390 verified, 60 failed\n",
      "Processed 475 samples (eps = 0.05): 413 verified, 62 failed\n",
      "Processed 500 samples (eps = 0.05): 438 verified, 62 failed\n",
      "Processed 525 samples (eps = 0.05): 461 verified, 64 failed\n",
      "Processed 550 samples (eps = 0.05): 484 verified, 66 failed\n",
      "Processed 575 samples (eps = 0.05): 507 verified, 68 failed\n",
      "Processed 600 samples (eps = 0.05): 532 verified, 68 failed\n",
      "Processed 625 samples (eps = 0.05): 557 verified, 68 failed\n",
      "Processed 650 samples (eps = 0.05): 581 verified, 69 failed\n",
      "Processed 675 samples (eps = 0.05): 606 verified, 69 failed\n",
      "Processed 700 samples (eps = 0.05): 631 verified, 69 failed\n",
      "Processed 725 samples (eps = 0.05): 655 verified, 70 failed\n",
      "Processed 750 samples (eps = 0.05): 680 verified, 70 failed\n",
      "Processed 775 samples (eps = 0.05): 704 verified, 71 failed\n",
      "Processed 800 samples (eps = 0.05): 728 verified, 72 failed\n",
      "Processed 825 samples (eps = 0.05): 752 verified, 73 failed\n",
      "Processed 836 samples (eps = 0.05): 763 verified, 73 failed; Avg. ReLU count: 0.23205741626794257\n"
     ]
    }
   ],
   "source": [
    "# Verify a set of points for base model\n",
    "model = tf.keras.models.load_model(\"dnn_base.h5\")\n",
    "epsilon = 0.05\n",
    "\n",
    "result = eps_ball_verify(\n",
    "    model=model,\n",
    "    X=scale_data(train_correct_malicious_features),\n",
    "    expected_label=MALICIOUS,\n",
    "    eps=epsilon,\n",
    "    threshold=threshold,\n",
    "    features=train_correct_malicious_features.columns,\n",
    "    free_features=free_features,\n",
    "    extra_constraints=constraints, \n",
    "    debug=True)\n",
    "\n",
    "_ = gc.collect()\n",
    "# 6m 31.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 25 samples (eps = 0.15): 21 verified, 4 failed\n",
      "Processed 50 samples (eps = 0.15): 37 verified, 13 failed\n",
      "Processed 75 samples (eps = 0.15): 53 verified, 22 failed\n",
      "Processed 100 samples (eps = 0.15): 72 verified, 28 failed\n",
      "Processed 125 samples (eps = 0.15): 93 verified, 32 failed\n",
      "Processed 150 samples (eps = 0.15): 117 verified, 33 failed\n",
      "Processed 175 samples (eps = 0.15): 136 verified, 39 failed\n",
      "Processed 200 samples (eps = 0.15): 157 verified, 43 failed\n",
      "Processed 225 samples (eps = 0.15): 174 verified, 51 failed\n",
      "Processed 250 samples (eps = 0.15): 194 verified, 56 failed\n",
      "Processed 275 samples (eps = 0.15): 215 verified, 60 failed\n",
      "Processed 300 samples (eps = 0.15): 235 verified, 65 failed\n",
      "Processed 325 samples (eps = 0.15): 256 verified, 69 failed\n",
      "Processed 350 samples (eps = 0.15): 279 verified, 71 failed\n",
      "Processed 375 samples (eps = 0.15): 301 verified, 74 failed\n",
      "Processed 400 samples (eps = 0.15): 325 verified, 75 failed\n",
      "Processed 425 samples (eps = 0.15): 347 verified, 78 failed\n",
      "Processed 450 samples (eps = 0.15): 370 verified, 80 failed\n",
      "Processed 475 samples (eps = 0.15): 393 verified, 82 failed\n",
      "Processed 500 samples (eps = 0.15): 416 verified, 84 failed\n",
      "Processed 525 samples (eps = 0.15): 434 verified, 91 failed\n",
      "Processed 550 samples (eps = 0.15): 454 verified, 96 failed\n",
      "Processed 575 samples (eps = 0.15): 475 verified, 100 failed\n",
      "Processed 600 samples (eps = 0.15): 500 verified, 100 failed\n",
      "Processed 625 samples (eps = 0.15): 522 verified, 103 failed\n",
      "Processed 650 samples (eps = 0.15): 543 verified, 107 failed\n",
      "Processed 675 samples (eps = 0.15): 565 verified, 110 failed\n",
      "Processed 700 samples (eps = 0.15): 587 verified, 113 failed\n",
      "Processed 725 samples (eps = 0.15): 610 verified, 115 failed\n",
      "Processed 750 samples (eps = 0.15): 634 verified, 116 failed\n",
      "Processed 775 samples (eps = 0.15): 656 verified, 119 failed\n",
      "Processed 800 samples (eps = 0.15): 677 verified, 123 failed\n",
      "Processed 825 samples (eps = 0.15): 701 verified, 124 failed\n",
      "Processed 836 samples (eps = 0.15): 711 verified, 125 failed; Avg. ReLU count: 0.8361244019138756\n"
     ]
    }
   ],
   "source": [
    "# Verify a set of points for base model\n",
    "model = tf.keras.models.load_model(\"dnn_base.h5\")\n",
    "epsilon = 0.15\n",
    "\n",
    "result = eps_ball_verify(\n",
    "    model=model,\n",
    "    X=scale_data(train_correct_malicious_features),\n",
    "    expected_label=MALICIOUS,\n",
    "    eps=epsilon,\n",
    "    threshold=threshold,\n",
    "    features=train_correct_malicious_features.columns,\n",
    "    free_features=free_features,\n",
    "    extra_constraints=constraints, \n",
    "    debug=True)\n",
    "\n",
    "_ = gc.collect()\n",
    "# 92m 22.7s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "93m 25s - 711 verified, 125 failed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botnet-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
